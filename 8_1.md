Homework 8
================

Problem 8.1
-----------

### Problem 8.1.1

``` r
library(alr4)
```

``` r
plot(Tension ~ Sulfur, data = baeskel, smooth = TRUE, spread = FALSE)
```

![](8_1_files/figure-markdown_github/unnamed-chunk-2-1.png)

The points appear to fall on a curved line, not a straight line. Hence, we can say that a transformation is required to achieve a straight line mean function.

### Problem 8.1.2

``` r
plot(Tension ~ Sulfur, baeskel)
grid(col="gray", lty="solid")
m1 <- lm(Tension ~ Sulfur, baeskel)
abline(m1, lwd=1)
new <- with(baeskel, seq(min(Sulfur), max(Sulfur), length=100))
m2 <- update(m1, ~ I(1/Sulfur))
with(baeskel, lines(new, predict(m2, data.frame(Sulfur=new)), lwd=1.5, lty=2))
m2 <- update(m2, ~ log(Sulfur))
with(baeskel, lines(new, predict(m2, data.frame(Sulfur=new)), lwd=2, lty=3))
legend("bottomright", c(" 1", "-1", " 0"), lwd=c(1, 1.5, 2), lty=1:3, inset=0.02)
```

![](8_1_files/figure-markdown_github/unnamed-chunk-3-1.png)

We can easily see from the above figure that the log transformation (lambda = 0) most closely fits the data.

We can plot this using invTranPlot function, which also displays the RSS for each value of lambda.

``` r
with(baeskel, invTranPlot(Sulfur, Tension))
```

![](8_1_files/figure-markdown_github/unnamed-chunk-4-1.png)

    ##     lambda       RSS
    ## 1  0.03442  2484.107
    ## 2 -1.00000 35691.735
    ## 3  0.00000  2535.896
    ## 4  1.00000 35824.332

### Problem 8.1.3

The inverse response plot is equivalent to a plot of the response on the horizontal axis and the predictor on the vertical axis. We can make this plot with the invResPlot function.

``` r
invResPlot(lm(Tension ~ log(Sulfur), baeskel))
```

![](8_1_files/figure-markdown_github/unnamed-chunk-5-1.png)

    ##       lambda       RSS
    ## 1  0.6860853  2202.113
    ## 2 -1.0000000 10594.340
    ## 3  0.0000000  3658.171
    ## 4  1.0000000  2509.564

We see that lambda = 1 matches quite well. So we don't need to transform any further.

Problem 8.3
-----------

### Problem 8.3.1

Constructing the scatterplot matrix

``` r
pairs(water)
```

![](8_1_files/figure-markdown_github/unnamed-chunk-6-1.png)

Conclusions:

1.  The "O" measurements are very highly correlated, but the "A" measurements are less highly correlated.
2.  Correlations between O and A variables are small.
3.  There is no obvious dependence on time
4.  Evidence of curvature in the marginal response plots, the last row of the scatterplot matrix, is weak.
5.  Marginally the O variables appear to be more highly correlated with the response than are the A variables.

### Problem 8.3.2

``` r
summary(ans <- powerTransform( as.matrix(water[ , 2:7]) ~ 1))
```

    ## bcPower Transformations to Multinormality 
    ##         Est Power Rounded Pwr Wald Lwr bnd Wald Upr Bnd
    ## APMAM      0.0982           0      -0.4625       0.6589
    ## APSAB      0.3450           0      -0.0533       0.7432
    ## APSLAKE    0.0818           0      -0.3466       0.5101
    ## OPBPC      0.0982           0      -0.2109       0.4073
    ## OPRC       0.2536           0      -0.2255       0.7328
    ## OPSLAKE    0.2534           0      -0.0921       0.5988
    ## 
    ## Likelihood ratio tests about transformation parameters
    ##                                       LRT df         pval
    ## LR test, lambda = (0 0 0 0 0 0)  5.452999  6 4.871556e-01
    ## LR test, lambda = (1 1 1 1 1 1) 61.203125  6 2.562905e-11

We should transform all predictors to the log scale, since the p-value for the LR test is about 0.49

### Problem 8.3.3

The inverse response plot method will indicate that the log transformation matches the data.

``` r
m4 <- lm(BSAAM ~ log(APMAM) + log(APSAB) + log(APSLAKE) +
log(OPBPC) + log(OPRC) + log(OPSLAKE), water)
invResPlot(m4)
```

![](8_1_files/figure-markdown_github/unnamed-chunk-8-1.png)

    ##       lambda        RSS
    ## 1  0.1048461 2257433456
    ## 2 -1.0000000 3008670148
    ## 3  0.0000000 2264377190
    ## 4  1.0000000 2745251921

The lines shown on the plot are for lambda\_hat = .10, the nonlinear LS estimate, and for lambda = 0, 1. The fitted line matches the fitted line for lambda = 0 almost perfectly.

Problem 8.5
-----------

### Problem 8.5.1

``` r
plot(BigMac ~ FoodIndex, data = BigMac2003, id.n = 2)
abline(lm(BigMac ~ FoodIndex, data = BigMac2003))
```

![](8_1_files/figure-markdown_github/unnamed-chunk-9-1.png)

``` r
# Find top two cities with highest BigMac prices
d<-BigMac2003[order(-BigMac2003$BigMac), , drop = FALSE]

d[1:2,]
```

    ##         BigMac Bread Rice FoodIndex  Bus Apt TeachGI TeachNI TaxRate
    ## Nairobi    185    45   70      45.6 0.32 320     1.9     1.6 15.7895
    ## Karachi    132    62   46      35.7 0.22  90     2.0     1.8 10.0000
    ##         TeachHours
    ## Nairobi         43
    ## Karachi         40

The scatterplot indicates that the real cost of a Big Mac, the amount of work required to buy one, declines with overall food prices; the Big Mac is cheapest, for the local people, in countries with high FoodIndex. The cost in Nairobi and Karachi were relatively very high. In Nairobi and Karachi, 185 and 132 minutes of labor respectively are required by the typical worker to buy a Big Mac.

### Problem 8.5.2

Box-Cox

``` r
m1 <- lm(BigMac ~ FoodIndex, BigMac2003)
summary(powerTransform(m1))
```

    ## bcPower Transformation to Normality 
    ##    Est Power Rounded Pwr Wald Lwr bnd Wald Upr Bnd
    ## Y1   -0.4471        -0.5      -0.7478      -0.1463
    ## 
    ## Likelihood ratio tests about transformation parameters
    ##                            LRT df        pval
    ## LR test, lambda = (0)  8.85337  1 0.002925483
    ## LR test, lambda = (1) 93.46308  1 0.000000000

Inverse Response plot

``` r
invResPlot(m1, id.n=2, id.method="x")
```

![](8_1_files/figure-markdown_github/unnamed-chunk-11-1.png)

    ##       lambda      RSS
    ## 1 -0.5841499 10251.99
    ## 2 -1.0000000 10527.52
    ## 3  0.0000000 10907.02
    ## 4  1.0000000 14846.40

Both methods suggest using the inverse square root scale for BigMac, although the improvement over the logarithmic transformation is small.

### Problem 8.5.3

``` r
sel <- match(c("Karachi", "Nairobi"), rownames(BigMac2003))
m2 <- update(m1, subset=-sel)
invResPlot(m2, id.n=2, id.method="x")
```

![](8_1_files/figure-markdown_github/unnamed-chunk-12-1.png)

    ##       lambda      RSS
    ## 1 -0.3671045 7272.396
    ## 2 -1.0000000 7616.896
    ## 3  0.0000000 7394.709
    ## 4  1.0000000 8753.747

``` r
summary(powerTransform(m2))
```

    ## bcPower Transformation to Normality 
    ##    Est Power Rounded Pwr Wald Lwr bnd Wald Upr Bnd
    ## Y1   -0.3342           0       -0.682       0.0136
    ## 
    ## Likelihood ratio tests about transformation parameters
    ##                             LRT df         pval
    ## LR test, lambda = (0)  3.564035  1 5.904405e-02
    ## LR test, lambda = (1) 54.012681  1 1.991740e-13

Although the estimated power is now close to the inverse cube root, both methods suggest little diference between the best estimate and logarithms. Logs are much easier to interpret and should be used in this problem.

### Problem 8.5.4

``` r
pairs(~ BigMac + Rice + Bread, BigMac2003)
```

![](8_1_files/figure-markdown_github/unnamed-chunk-14-1.png)

The scatterplot matrix indicates the need to transform because the points are clustered on the lower-left corners of the plots, the variables range over several orders of magnitude and the curvature is apparent.

Box-Cox

``` r
summary(pows <- powerTransform(cbind(BigMac, Rice, Bread) ~ 1, BigMac2003))
```

    ## bcPower Transformations to Multinormality 
    ##        Est Power Rounded Pwr Wald Lwr bnd Wald Upr Bnd
    ## BigMac   -0.3035        -0.5      -0.5980      -0.0089
    ## Rice     -0.2406         0.0      -0.5043       0.0230
    ## Bread    -0.1566         0.0      -0.4439       0.1307
    ## 
    ## Likelihood ratio tests about transformation parameters
    ##                                  LRT df       pval
    ## LR test, lambda = (0 0 0)   7.683155  3 0.05303454
    ## LR test, lambda = (1 1 1) 204.555613  3 0.00000000

We remove two cases to get:

``` r
summary(pow1s<-powerTransform(cbind(BigMac, Rice, Bread) ~ 1, BigMac2003,
subset=-c(26, 46)))
```

    ## bcPower Transformations to Multinormality 
    ##        Est Power Rounded Pwr Wald Lwr bnd Wald Upr Bnd
    ## BigMac   -0.2886           0      -0.6301       0.0529
    ## Rice     -0.2465           0      -0.5235       0.0305
    ## Bread    -0.1968           0      -0.4922       0.0986
    ## 
    ## Likelihood ratio tests about transformation parameters
    ##                                  LRT df       pval
    ## LR test, lambda = (0 0 0)   7.083917  3 0.06927061
    ## LR test, lambda = (1 1 1) 181.891304  3 0.00000000

The resulting transformations are not very different from the transformations using all the data, and logs of all three seem to be appropriate. The scatterplot matrix for the transformed variables is:

``` r
pairs(~ log(BigMac) + log(Rice) + log(Bread), BigMac2003)
```

![](8_1_files/figure-markdown_github/unnamed-chunk-17-1.png)

### Problem 8.5.5

``` r
m2 <- lm(BigMac ~ I(Apt^(1/3)) + log(Bread) + log(Bus) + log(TeachGI), BigMac2003)
invResPlot(m2, xlab="(a) BigMac")
```

![](8_1_files/figure-markdown_github/unnamed-chunk-18-1.png)

    ##       lambda       RSS
    ## 1 -0.2569342  7863.537
    ## 2 -1.0000000 10340.719
    ## 3  0.0000000  8228.198
    ## 4  1.0000000 16321.128

The graph suggests a negative cube root or a logarithm transformation will be most suitable.

``` r
m3 <- update(m2, I(BigMac^(-1/3)) ~ .)
invResPlot(m3, xlab=expression(paste("(b) ", BigMac^(-1/3))))
```

![](8_1_files/figure-markdown_github/unnamed-chunk-19-1.png)

    ##       lambda        RSS
    ## 1  0.7997681 0.05645971
    ## 2 -1.0000000 0.07123477
    ## 3  0.0000000 0.05930982
    ## 4  1.0000000 0.05662967

As the inverse response plot is nearly linear, further transformations do not seem necessary.

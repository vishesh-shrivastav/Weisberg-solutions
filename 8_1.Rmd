---
title: "Homework 8"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
  fig.path = "README_figs/"
```

## Problem 8.1

### Problem 8.1.1
```{r, message=FALSE, warning=FALSE}
library(alr4)
```
```{r, message=FALSE, warning=FALSE}
plot(Tension ~ Sulfur, data = baeskel, smooth = TRUE, spread = FALSE)
```

The points appear to fall on a curved line, not a straight line. Hence, we can say that a transformation is required to achieve a straight line mean function.

### Problem 8.1.2
```{r, warning=FALSE}
plot(Tension ~ Sulfur, baeskel)
grid(col="gray", lty="solid")
m1 <- lm(Tension ~ Sulfur, baeskel)
abline(m1, lwd=1)
new <- with(baeskel, seq(min(Sulfur), max(Sulfur), length=100))
m2 <- update(m1, ~ I(1/Sulfur))
with(baeskel, lines(new, predict(m2, data.frame(Sulfur=new)), lwd=1.5, lty=2))
m2 <- update(m2, ~ log(Sulfur))
with(baeskel, lines(new, predict(m2, data.frame(Sulfur=new)), lwd=2, lty=3))
legend("bottomright", c(" 1", "-1", " 0"), lwd=c(1, 1.5, 2), lty=1:3, inset=0.02)
```

We can easily see from the above figure that the log transformation (lambda = 0) most closely fits the data. 

We can plot this using invTranPlot function, which also displays the RSS for each value of lambda.

```{r, warning=FALSE}
with(baeskel, invTranPlot(Sulfur, Tension))
```

### Problem 8.1.3
The inverse response plot is equivalent to a plot of the response on the horizontal axis and the predictor on the vertical axis. We can make this plot with the invResPlot function.
```{r, warning=FALSE}
invResPlot(lm(Tension ~ log(Sulfur), baeskel))
```

We see that lambda = 1 matches quite well. So we don't need to transform any further.

## Problem 8.3

### Problem 8.3.1

Constructing the scatterplot matrix

```{r, warning=FALSE}
pairs(water)
```

Conclusions:

1. The "O" measurements are very highly correlated, but the "A" measurements are less highly correlated.
2. Correlations between O and A variables are small.
3. There is no obvious dependence on time
4. Evidence of curvature in the marginal response plots, the last row of the scatterplot matrix, is weak.
5. Marginally the O variables appear to be more highly correlated with the response than are the A variables.

### Problem 8.3.2
```{r, warning=FALSE}
summary(ans <- powerTransform( as.matrix(water[ , 2:7]) ~ 1))
```

We should transform all predictors to the log scale, since the p-value for the LR test is about 0.49

### Problem 8.3.3
The inverse response plot method will indicate that the log transformation matches the data.
```{r, warning=FALSE}
m4 <- lm(BSAAM ~ log(APMAM) + log(APSAB) + log(APSLAKE) +
log(OPBPC) + log(OPRC) + log(OPSLAKE), water)
invResPlot(m4)
```

The lines shown on the plot are for lambda_hat = .10, the nonlinear LS estimate,
and for lambda = 0, 1. The fitted line matches the fitted line for lambda = 0 almost perfectly.

## Problem 8.5

### Problem 8.5.1
```{r, warning=FALSE}
plot(BigMac ~ FoodIndex, data = BigMac2003, id.n = 2)
abline(lm(BigMac ~ FoodIndex, data = BigMac2003))

# Find top two cities with highest BigMac prices
d<-BigMac2003[order(-BigMac2003$BigMac), , drop = FALSE]

d[1:2,]
```

The scatterplot indicates that the real cost of a Big Mac, the amount of work required to buy one, declines with overall food prices; the Big Mac is cheapest, for the local people, in countries with high FoodIndex. The cost in Nairobi and Karachi were relatively very high. In Nairobi and Karachi, 185 and 132 minutes of labor respectively are required by the typical worker to buy a Big Mac.

### Problem 8.5.2
Box-Cox
```{r, warning=FALSE}
m1 <- lm(BigMac ~ FoodIndex, BigMac2003)
summary(powerTransform(m1))
```

Inverse Response plot
```{r, warning=FALSE}
invResPlot(m1, id.n=2, id.method="x")
```

Both methods suggest using the inverse square root scale for BigMac, although the improvement over the logarithmic transformation is small.

### Problem 8.5.3
```{r, warning=FALSE}
sel <- match(c("Karachi", "Nairobi"), rownames(BigMac2003))
m2 <- update(m1, subset=-sel)
invResPlot(m2, id.n=2, id.method="x")
```

```{r, warning=FALSE}
summary(powerTransform(m2))
```

Although the estimated power is now close to the inverse cube root, both methods suggest little diference between the best estimate and logarithms. Logs are much easier to interpret and should be used in this problem.

### Problem 8.5.4

```{r, warning=FALSE}
pairs(~ BigMac + Rice + Bread, BigMac2003)
```

The scatterplot matrix indicates the need to transform because the points are clustered on the lower-left corners of the plots, the variables range over several orders of magnitude and the curvature is apparent.

Box-Cox
```{r, warning=FALSE}
summary(pows <- powerTransform(cbind(BigMac, Rice, Bread) ~ 1, BigMac2003))
```

We remove two cases to get:
```{r, warning=FALSE}
summary(pow1s<-powerTransform(cbind(BigMac, Rice, Bread) ~ 1, BigMac2003,
subset=-c(26, 46)))
```

The resulting transformations are not very different from the transformations using all the data, and logs of all three seem to be appropriate. The scatterplot matrix for the transformed variables is:
```{r, warning=FALSE}
pairs(~ log(BigMac) + log(Rice) + log(Bread), BigMac2003)
```

### Problem 8.5.5
```{r, warning=FALSE}
m2 <- lm(BigMac ~ I(Apt^(1/3)) + log(Bread) + log(Bus) + log(TeachGI), BigMac2003)
invResPlot(m2, xlab="(a) BigMac")
```

The graph suggests a negative cube root or a logarithm transformation will be most suitable.
```{r, warning=FALSE}
m3 <- update(m2, I(BigMac^(-1/3)) ~ .)
invResPlot(m3, xlab=expression(paste("(b) ", BigMac^(-1/3))))
```

As the inverse response plot is nearly linear, further transformations do not seem necessary.
